# Deep Neural Networks (L5+L6+L8)

## 0 outline

**L5 Deep Neural Network I_DNN**

1. building nn
2. training nn
3. advanced technique
4. batch normalization, â€¦â€¦

**L6 Deep Neural Network Il CNN(Convolutional)**

1. concept
2. architecture
3. Application
4. Important networks
5. deep learning program tutorial: pytorch

**L8 Deep Neural Network Il -RNN (Recurrent)**

1. concept +architeture
2. LSTM (long short-term memory RNNS)
3. Transformer
   3.1 Attention in DL
   3.2 Attention in Transformer
   query, key, value
   self-attention
   positional encoding
   multi-head
4. Transformer VS RNN
5. Advanced: ViT or CNN
   ViT
   Swin Transformer : Tailored ViT


---

### L5 Deep Neural Network I __ DNN

#### 1. building nn: 

1. perceptron   (weight bias activation function)
2. forward propagation
3. activation function

#### 2. trainning nn:

1. backpropagation of error (with GD)

   ![image-20231030164558677](.\asset\20231016_bp.png)
   
   * pros -- **Re-use derivatives** computed for higher layers in computing derivatives for lower layers to minimize computation
   * cons -- **Technical** view: trapped into **local minima** 

#### 3. advanced techiques

practically training DNN with BP is difficult ;  Loss functions can be difficult to optimize + Dealing with overfitting

1. adaptive **learning rate**

2. **advanced GD**: momentum  RMSProp Adam  (å…·ä½“è§è§†å¬å¯¼ç¬”è®°ä¼˜åŒ–éƒ¨åˆ†)

3. **regularization**: L1/L2 , dropout,early stopping

   * **Regularization:**Prevent the model from doing too well on  training data

   ![image-20231106141403826](.\asset\20231016_regularization_l1l2.png)

   * **dropout** : During training, randomly set some activations to 0
   * **early stopping**: Stop training before having a chance to overfit

####  4. batch normalization, hyperparameters, initialization

![image-20231106141646406](.\asset\20231016_batch_normalization.png)

* **batchnorm**ä½¿å¾—ç½‘ç»œä¸­æ¯å±‚è¾“å…¥æ•°æ®çš„åˆ†å¸ƒç›¸å¯¹ç¨³å®šï¼ŒåŠ é€Ÿæ¨¡å‹å­¦ä¹ é€Ÿåº¦
* **batchnorm**ä½¿å¾—æ¨¡å‹å¯¹ç½‘ç»œä¸­çš„å‚æ•°ä¸é‚£ä¹ˆæ•æ„Ÿï¼Œç®€åŒ–è°ƒå‚è¿‡ç¨‹ï¼Œä½¿å¾—ç½‘ç»œå­¦ä¹ æ›´åŠ ç¨³å®š
  * æƒé‡çš„ç¼©æ”¾å€¼ä¼šè¢«â€œæŠ¹å»â€ï¼Œå› æ­¤ä¿è¯äº†è¾“å…¥æ•°æ®åˆ†å¸ƒç¨³å®šåœ¨ä¸€å®šèŒƒå›´å†…ã€‚
* **batchnorm**å…è®¸ç½‘ç»œä½¿ç”¨é¥±å’Œæ€§æ¿€æ´»å‡½æ•°ï¼ˆä¾‹å¦‚sigmoidï¼Œtanhç­‰ï¼‰ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜
  * åœ¨ä¸ä½¿ç”¨**batchnorm**å±‚çš„æ—¶å€™ï¼Œç”±äºç½‘ç»œçš„æ·±åº¦ä¸å¤æ‚æ€§ï¼Œå¾ˆå®¹æ˜“ä½¿å¾—åº•å±‚ç½‘ç»œå˜åŒ–ç´¯ç§¯åˆ°ä¸Šå±‚ç½‘ç»œä¸­ï¼Œå¯¼è‡´æ¨¡å‹çš„è®­ç»ƒå¾ˆå®¹æ˜“è¿›å…¥åˆ°æ¿€æ´»å‡½æ•°çš„**æ¢¯åº¦é¥±å’ŒåŒº**ï¼›é€šè¿‡normalizeæ“ä½œå¯ä»¥è®©æ¿€æ´»å‡½æ•°çš„è¾“å…¥æ•°æ®è½åœ¨æ¢¯åº¦éé¥±å’ŒåŒºï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ï¼›å¦å¤–é€šè¿‡**è‡ªé€‚åº”**å­¦ä¹  ğ›¾, ğ›½ åˆè®©æ•°æ®ä¿ç•™æ›´å¤šçš„åŸå§‹ä¿¡æ¯ã€‚
*  **batchnorm**å…·æœ‰ä¸€å®šçš„æ­£åˆ™åŒ–æ•ˆæœ
  * ç”±äºä½¿ç”¨mini-batchçš„å‡å€¼ä¸æ–¹å·®ä½œä¸ºå¯¹æ•´ä½“è®­ç»ƒæ ·æœ¬å‡å€¼ä¸æ–¹å·®çš„ä¼°è®¡ï¼Œå°½ç®¡æ¯ä¸€ä¸ªbatchä¸­çš„æ•°æ®éƒ½æ˜¯ä»æ€»ä½“æ ·æœ¬ä¸­æŠ½æ ·å¾—åˆ°ï¼Œä½†**ä¸åŒmini-batchçš„å‡å€¼ä¸æ–¹å·®ä¼šæœ‰æ‰€ä¸åŒ**ï¼Œä¸ºç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹ä¸­å¢åŠ äº†éšæœºå™ªéŸ³ï¼Œä¸Dropouté€šè¿‡å…³é—­ç¥ç»å…ƒç»™ç½‘ç»œè®­ç»ƒå¸¦æ¥å™ªéŸ³ç±»ä¼¼ï¼Œåœ¨ä¸€å®šç¨‹åº¦ä¸Šå¯¹æ¨¡å‹èµ·åˆ°äº†æ­£åˆ™åŒ–çš„æ•ˆæœã€‚
  * æ³¨æ„è¿™é‡Œç”¨æ¥å½’ä¸€å¤„ç†çš„å‡å€¼å’Œæ–¹å·®æ¥è‡ªäºæœ¬æ‰¹æ¬¡çš„æ•°æ®ï¼Œè€Œä¸æ˜¯å…¨éƒ¨æ•°æ®ã€‚

* **A few considerations about batch norm:**
  * Often a **larger learning rate** with batch norm is used
  * Models with batch norm can train **much faster**
  * Generally **requires less regularization** (e.g. w/o. dropout)
  * Good idea in many cases

* hyperparameters   
  * **Optimization related:**
    * batch size
    * learning rate
    * momentum
    * initialization
    * batch normalization
  * **Generalization related:**
    * Dropout
  * **Both related:**
    * Architecture
    * and size of layers
    * activation

---

### L6 Deep Neural Network II __CNN(Convolutional)

#### 1. concept

* convolution: exploit spatial structures and hierachical pattern

  * shared connections / spatial filtering 

*  stack of Conv, Pool, FC Layers

  * smaller filters and deeper architectures
  * getting rid of fully concolutional layers

  * local connectivity (fully connectivity: MLP)

  * receptive field (the  region of visual space): hierachical organization

#### 2. architecture

* strided convolution: ouput size: **(N - F) / stride + 1**\
* zero padding:  **(N + 2P - F) / stride + 1**
* pooling: max / average pooling
* FC: fully convolutional

#### 3. Application

feature learning + classification

![image-20231030170214698](.\asset\20231016_cnn_architecture_1.png)



![image-20231030170251063](.\asset\20231016_cnn_architecture_2.png)

A practical issue: insufficient training data for very deep network?

Solution: **data augmentation**

![image-20231030170402023](.\asset\20231016_cnn_architecture_3.png)

* rotation: uniformly chosen random **angle** between 0Â° and 360Â°
* translation: random **translation** between -10 and 10 pixels
* rescaling: random **scaling** with scale factor between 1/1.6 and 1.6
* flipping: yes or no (bernoulli)
* shearing: random **shearing** with angle between -20Â° and 20Â° ï¼ˆè£å‰ª
* stretching: random **stretching** with stretch factor between 1/1.3 and 1.3
* cropping: **crop a for random range** from the center
* color jitter: hue, saturation, brightness, contrast, â€¦
* add noise, blur, cutout, â€¦**

#### 4. Important  networks:

* **AlexNet(8 layers):** 
  * heavy data augmentation, SGDMomentum, L2weight decay
  * **first ReLu**æ˜¾è‘—æé«˜äº†æ”¶æ•›é€Ÿåº¦ã€‚
  * å®ç°å¹¶å¼€æºäº† cuda-convnetï¼Œä½¿å¾—åœ¨ **GPU** **ä¸Šè®­ç»ƒå¤§è§„æ¨¡ç¥ç»ç½‘ç»œ**å˜å¾—å¯è¡Œã€‚
* **VGGNet(16/19 layers):**
  *  receptive field
    * ä¸åŒå±‚æ¬¡çš„ç‰¹å¾ä¹‹é—´æœ‰ä¸€ä¸ªç®€å•çš„æ¯”ä¾‹å…³ç³»

  * é€‚ç”¨äºä½ç½®æ•æ„Ÿçš„ä¸‹æ¸¸ä»»åŠ¡ï¼Œå¦‚æ£€æµ‹ã€åˆ†å‰²ç­‰
  * more non-linearitiesï¼ˆ**Small** filters, **Deeper** networksï¼‰
    * å……åˆ†ä½¿ç”¨**3Ã—3 çš„å·ç§¯æ ¸ï¼Œ**æ­¥é•¿1å’Œå¡«å……1ï¼Œä»¥ä¿æŒç©ºé—´ç»´åº¦ä¸å˜
      * ä¸‰ä¸ª 3x3 çš„å·ç§¯å±‚ï¼ˆæ­¥å¹…ä¸º 1ï¼‰å †å å…·æœ‰ä¸ä¸€ä¸ª 7x7 çš„å·ç§¯å±‚ç›¸åŒçš„æœ‰æ•ˆæ„Ÿå—è§†é‡â€”â€” æ›´å°çš„æ»¤æ³¢å™¨ï¼Œä½†æ›´æ·±ï¼Œéçº¿æ€§æ€§æ›´å¼º
      * å®ƒä¸ä»…è¯æ˜äº†æ·±åº¦å¯¹äºåˆ†ç±»å‡†ç¡®æ€§æ˜¯æœ‰ç›Šçš„ï¼Œè¿˜è¯æ˜äº†3x3å·ç§¯æ ¸è¶³å¤Ÿä¼˜ç§€ã€‚

    * æ‰€æœ‰éšè—å±‚éƒ½ä½¿ç”¨ **ReLU**

  * ç©ºé—´æ± åŒ–ç”±**äº”ä¸ªæœ€å¤§æ± åŒ–å±‚**å®Œæˆï¼Œä»¥ 2Ã—2 åƒç´ çª—å£æ‰§è¡Œï¼Œæ­¥å¹…ä¸º 2
  * å‰ä¸¤ä¸ª**å…¨è¿æ¥ï¼ˆFCï¼‰å±‚**å„æœ‰ 4096 ä¸ªé€šé“ï¼Œç¬¬ä¸‰ä¸ªæ‰§è¡Œ 1000 ç±»åˆ«çš„ ILSVRC åˆ†ç±»ï¼Œå› æ­¤åŒ…å« 1000 ä¸ªé€šé“ï¼ˆæ¯ä¸ªç±»åˆ«ä¸€ä¸ªé€šé“ï¼‰ã€‚
  * ç¼“è§£è¿‡æ‹Ÿåˆ
    * å›¾åƒå¢å¼ºï¼š224x224 è¾“å…¥æ˜¯ä»è¾“å…¥å›¾ç‰‡ä¸­éšæœºè£å‰ªçš„ï¼Œéšæœºæ°´å¹³ç¿»è½¬å’Œéšæœº RGB é¢œè‰²åç§»
    * dropoutï¼šå‰ä¸¤ä¸ªå…¨è¿æ¥å±‚çš„ dropout æ­£åˆ™åŒ–

  * SGDï¼šå¤§åŠ¨é‡ï¼Œå°æƒé‡è¡°å‡

* GooleNet(22 layers): 
  * æ›´æ·±ï¼Œæ›´é«˜æ•ˆ
  * **parallel filter** 
    * å¯¹æ¥è‡ªå‰ä¸€å±‚çš„è¾“å…¥åº”ç”¨å¹¶è¡Œçš„æ»¤æ³¢æ“ä½œï¼šå·ç§¯å¤šä¸ªæ„Ÿå—é‡å°ºå¯¸
    * ä»¥åˆå¹¶ç‰¹å¾é€šé“çš„æ–¹å¼å°†æ‰€æœ‰æ»¤æ³¢å™¨è¾“å‡ºè¿æ¥åœ¨ä¸€èµ·ã€‚
    * æ± åŒ–å±‚è¿˜ä¿ç•™äº†åŸé€šé“æ•°ï¼Œè¿™æ„å‘³ç€è¿æ¥åçš„æ€»é€šé“æ•°åªèƒ½ä¸æ–­å¢é•¿
      * è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ 1x1 å·ç§¯æ¥å‡å°‘ç‰¹å¾é€šé“å¤§å°çš„â€œç“¶é¢ˆâ€å±‚ï¼ˆä¿ç•™ç©ºé—´ç»´åº¦ï¼Œå‡å°‘é€šé“æ•°ï¼‰ï¼ˆä½¿ç”¨äº† 1x1 çš„ç“¶é¢ˆå·ç§¯å’Œå…¨å±€å¹³å‡æ± åŒ–ä»£æ›¿å…¨è¿æ¥å±‚ï¼‰
      * ![image-20240113205654470](.\asset\bottleneck.png)

  * é‡‡ç”¨ **Inception** **æ¨¡å—**
    * è®¾è®¡ä¸€ä¸ªè‰¯å¥½çš„å±€éƒ¨ç½‘ç»œæ‹“æ‰‘ç»“æ„ï¼ˆç½‘ç»œå†…çš„ç½‘ç»œï¼‰ï¼Œç„¶åå°†è¿™äº›æ¨¡å—å åŠ åœ¨ä¸€èµ·ï¼›è¾…åŠ©åˆ†æ”¯ä¹Ÿè¿›è¡Œåˆ†ç±»çš„è®­ç»ƒï¼Œå‘ç½‘ç»œä¸­ä¼ é€’æ›´å¤šçš„æ¢¯åº¦
    * **Inception v1:**
      * éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰
      * åŠ¨é‡ (Momentum) 0.9    å­¦ä¹ ç‡æ¯8è½®ä¸‹é™4%
      * é€šè¿‡è¾…åŠ©æ—è·¯åˆ†ç±»ç»“æœä¼ æ’­æ›´å¤šæ¢¯åº¦

    * **Inception v2:**
      * ç”¨ä¸¤ä¸ª3x3å·ç§¯ä»£æ›¿5x5å·ç§¯ï¼ˆå‡å°‘å‚æ•°é‡ã€å‡è½»è¿‡æ‹Ÿåˆï¼‰
      * ä½¿ç”¨äº†**Batch Normalization**ï¼Œæ˜¾ç¤ºäº†BNçš„ä¼˜å¼‚æ€§èƒ½ è‡ªæ­¤ä¹‹åï¼ŒBNä½œä¸ºæ­£åˆ™åŒ–æ–¹å¼è¢«å¤§é‡ä½¿ç”¨

    * **Inception v3:**
      * å°†nxnå·ç§¯æ‹†æˆ1xnå’Œnx1å·ç§¯çš„ç»“åˆï¼ˆèŠ‚çº¦å‚æ•°ã€åŠ é€Ÿè®¡ç®—ã€å¢åŠ éçº¿æ€§ï¼‰

    * **Inception v4:** ç»“åˆResNet

  * åˆ†ç±»å±‚ä»…åŒ…å« 1 ä¸ªå…¨è¿æ¥å±‚ï¼Œå‡å°‘å‚æ•°æ•°é‡ **7M** (AlexNet 62Mã€VGG 138M)

* ResNet:
  *  **residual net** ï¼šAdd more **direct connections** (thus enabling easier gradient flow towards lower layers)
  *  ä½¿ç”¨ç½‘ç»œå±‚æ¥æ‹Ÿåˆæ®‹å·®æ˜ å°„ï¼Œè€Œä¸æ˜¯ç›´æ¥å°è¯•æ‹Ÿåˆæ‰€éœ€çš„åº•å±‚æ˜ å°„
* Deformable ConvNet å¯å½¢å˜å·ç§¯
  * å­¦ä¹ å¦‚ä½•åœ¨å·ç§¯ä¸­å¯¹é‡‡æ ·ä½ç½®è¿›è¡Œå½¢å˜
  * å®ç°å·ç§¯ç¥ç»ç½‘ç»œä¸­**å¯¹ç©ºé—´å˜æ¢çš„æœ‰æ•ˆå»ºæ¨¡**
    * æ— éœ€é¢å¤–ç›‘ç£æ¥å­¦ä¹ ç©ºé—´å˜æ¢
    * åœ¨å¤æ‚çš„è§†è§‰ä»»åŠ¡ä¸­æ˜¾è‘—æé«˜å‡†ç¡®æ€§

* GAN(generative adversarial networks): to sample from simple distribution, learn **transformation** to train distribution;  use a discriminator network to tell  ("two- player game "---descriminator & generator)
  * Discriminator**: **distinguish between real and fake images (convolution network)
  * Generator**: **fool the discriminator by generating real-looking images (unsampling network with fractionally strided convolutions)
* **CNNs Outperform MLPs on Images?**
  * CNNs: **local connectivity**, exploit spatial structures and hierarchical pattern in data
  * MLPs: **fully connectivity** (each neuron in one layer is connected to **all** neurons in the next layer, easy to overfit)
* **GAN**   (remain)
  * Goodfellow, Ian, et al. "Generative adversarial networks." *Communications of the ACM* 63.11 (2020): 139-144.

![image-20231106144106685](.\asset\20231023_GAN.png)

![image-20231106144234998](.\asset\20231023_GAN2.png)

regression problem

#### 5. deep learning program tutorial: pytorch

**Diffusion models** are currently exploding state-of-the-art beating GANs

---

### L8 Deep Neural Network III â€”â€”RNN ï¼ˆRecurrentï¼‰

#### 1. concept +architeture

* **Sequential** modeling: **previous hidden states affect** subsequent hidden states

* Process input sequences of **arbitrary length**

![image-20231106144540296](.\asset\20231106_RNN.png)

éšè—å±‚ä¸æ–­ç»“åˆä»¥å‰çš„éšè—å±‚å’Œå½“å‰è¾“å…¥æ¥æ›´æ–°ï¼Œè¾“å…¥é€šè¿‡å¯¹éšè—å±‚softmaxå½’ä¸€åŒ–å¾—åˆ°ã€‚
$$
loss\space fuction:\space J(\theta)=\frac{1}{T}\sum_{t=1}^T J^{(t)}(\theta)\\
=-\frac{1}{T}\sum_{t=1}^T\sum_{i=1}^N y_i^{t}\space log\widehat y_i^{(t)}
$$
![image-20231106144758775](.\asset\20231106_RNN_BP.png)

è¿™é‡Œæ¯ä¸€æ—¶åˆ»çš„æŸå¤±å‡½æ•°æ˜¯ä»å¼€å§‹æ—¶åˆ»ä¸€ç›´åˆ°å½“å‰æ—¶åˆ»è¾“å‡ºçš„äº¤å‰ç†µæŸå¤±ï¼Œåœ¨ä½¿ç”¨åå‘ä¼ æ’­è®¡ç®—çš„æ˜¯å¶çš„ï¼Œåº”è¯¥å°†æ¯ä¸€æ—¶åˆ»çš„å•ä¸ªè¾“å‡ºçš„äº¤å‰ç†µæŸå¤±ï¼Œå¯¹éšè—å±‚è¿›è¡Œæ±‚å¯¼ï¼ˆç”±äºé“¾å¼æ³•åˆ™æŒ‰é“ç†åº”è¯¥åŠ ä¸Šè¿™ä¸€æ—¶åˆ»çš„éšè—å±‚å¯¹äºç°åœ¨çš„éšè—å±‚çš„åå¯¼ï¼Œä½†æ˜¯å› ä¸ºéšå±‚ä¸€ç›´ä¸å˜ï¼Œæ‰€ä»¥è¿™ä¸€é¡¹æ˜¯1ï¼‰ï¼Œæœ€åç›¸åŠ ã€‚

* å¯¹äºRNNçš„è¯„ä»·
  * ç†è®ºä¸Šå¯ä»¥ä½¿ç”¨å¾ˆå¤šæ­¥ä¹‹å‰çš„ä¿¡æ¯ï¼Œå¤„ç†ä»»æ„é•¿åº¦çš„è¾“å…¥è€ŒåŒæ—¶æ¨¡å‹çš„è§„æ¨¡å¹¶ä¸ä¼šå› æ­¤è€Œå¢åŠ 
  * ä½†æ˜¯å®è·µä¸­ä¼šé‡åˆ°æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼Œå¹¶ä¸èƒ½è·å¾—å¤šæ­¥ä¹‹å‰çš„ä¿¡æ¯ï¼›åŒæ—¶å¾ªç¯è®¡ç®—æ˜¯å¾ˆæ…¢çš„

![image-20231106144838688](.\asset\20231106_prosandcons.png)

#### 2. LSTM (long short-term memory RNN)

* åŸºæœ¬ç»“æ„
  * Cell state: run straight down the entire chain, with only **minor linear** **interactions**. Itâ€™s very easy for information to just flow along it unchanged
  * The cell stores **long-term information** LSTM can **read**, **erase**, and **write** information from the cell
  * Gate: optionally let information go through cell state

* ![image-20231106145108677](.\asset\20231106_LSTM1.png)
* é—å¿˜é—¨å¤„ç†è¿‡å»çš„ä¿¡æ¯ï¼Œè¾“å…¥é—¨å¤„ç†æ–°ä¿¡æ¯

![image-20231106145129757](.\asset\20231106_LSTM2.png)

![image-20231106145307379](.\asset\20231106_LSTM3.png)

* gate: propobility    weight 

#### 3. Transformer

##### 3.1 **Attention in DL:**

 estimate attention vector, reflecting how strongly it is **correlated** with (â€œattends toâ€) other elements

task1: translation

![image-20231106150951515](.\asset\20231106_seq2seq.png)

* ä¸€èˆ¬RNNçš„ç¼–ç å™¨å’Œè§£ç å™¨ï¼šç¼–ç å™¨æ˜¯å¤„ç†è¾“å…¥åºåˆ—å¹¶å…¨å‹ç¼©æˆä¸€ä¸ªvectorï¼Œè¡¨è¾¾æ‰€æœ‰ä½ç½®æƒå€¼çš„ä¿¡æ¯context vectorï¼›è§£ç å™¨æ˜¯ç»“åˆåˆå§‹çŠ¶æ€å’Œcontext vectorä¾æ¬¡æ¥å¾—åˆ°è¾“å‡º
* æ”¹è¿›ï¼šå¸Œæœ›ä¸åŒçš„ä½ç½®çš„context vectorä¸åŒï¼ˆè™½ç„¶æ¯ä¸ªä½ç½®éƒ½éœ€è¦åŒ…å«ä¹‹å‰çš„ä¿¡æ¯ï¼‰

![image-20231106151102402](.\asset\20231106_attention.png)

* \alphaæ˜¯å½“å‰iä½ç½®å’Œtä½ç½® çš„ç›¸å…³ç¨‹åº¦ï¼ˆä½†æ˜¯æ˜¯ç”¨å‰ä¸€æ—¶åˆ»å¾—åˆ°è¿™ä¸ªä¿¡æ¯ï¼‰
* å†ç®€å•ä½¿ç”¨\alpha å’Œéšå±‚hi æ¥ç®—å½“å‰çš„context vector ï¼ˆç”¨çš„æ˜¯ç®€å•çš„ç½‘ç»œï¼‰ï¼Œè¿™æ ·å°±åŒ…å«äº†
* æ³¨ï¼šç›¸å½“äºè¾“å…¥ä¸€æ–¹é¢æˆä¸ºéšå±‚çš„åˆå§‹åŒ–ï¼Œä¸€æ–¹é¢æ˜¯attentionæœºåˆ¶ç”¨æ¥äº§ç”Ÿç›¸å…³æ€§çš„â€œqueryâ€çš„ä¾æ®

![image-20231106151821057](.\asset\20231106_attention2.png)

task2: visualization of attention weights

* **CNN+RNN pipeline** **with attention**
  * Attention idea: **new context vector** at every time step 
  * Each context vector will **attend to** different image regions
  * Multiple **query vectors**, each **query** creates a new output context vector

* ä»¥ä¸Šæ˜¯é¦–æ¬¡æå‡ºçš„attentionï¼Œä½†æ˜¯æ˜¯åŸºäºRNNï¼Œè¿˜ä¸æ˜¯transformer ï¼Œæ¥ä¸‹æ¥å¼•å…¥Q K V



##### 3.2 Attention in Transformer

###### query, key, value

* **å¯ä»¥å°†Attentionæœºåˆ¶çœ‹ä½œä¸€ç§è½¯å¯»å€ï¼ˆSoft Addressingï¼‰:**
  Sourceå¯ä»¥çœ‹ä½œå­˜å‚¨å™¨å†…å­˜å‚¨çš„å†…å®¹ï¼Œå…ƒç´ ç”±åœ°å€Keyå’Œå€¼Valueç»„æˆï¼Œå½“å‰æœ‰ä¸ªKey=Queryçš„æŸ¥è¯¢ï¼Œç›®çš„æ˜¯å–å‡ºå­˜å‚¨å™¨ä¸­å¯¹åº”çš„Valueå€¼ï¼Œé€šè¿‡Queryå’Œå­˜å‚¨å™¨å†…å…ƒç´ Keyçš„åœ°å€è¿›è¡Œç›¸ä¼¼æ€§æ¯”è¾ƒæ¥å¯»å€ï¼Œä¹‹æ‰€ä»¥è¯´æ˜¯è½¯å¯»å€ï¼ŒæŒ‡çš„ä¸åƒä¸€èˆ¬å¯»å€åªä»å­˜å‚¨å†…å®¹é‡Œé¢æ‰¾å‡ºä¸€æ¡å†…å®¹ï¼Œè€Œæ˜¯å¯èƒ½ä»æ¯ä¸ªKeyåœ°å€éƒ½ä¼šå–å‡ºå†…å®¹ï¼Œå–å‡ºå†…å®¹çš„é‡è¦æ€§æ ¹æ®Queryå’ŒKeyçš„ç›¸ä¼¼æ€§æ¥å†³å®šï¼Œä¹‹åå¯¹Valueè¿›è¡ŒåŠ æƒæ±‚å’Œï¼Œè¿™æ ·å°±å¯ä»¥è·å¾—æœ€ç»ˆè¾“å‡ºçš„Valueå€¼ã€‚

  * åœ¨æœºå™¨ç¿»è¯‘çš„ä»»åŠ¡ä¸­ï¼ŒSourceä¸­çš„Keyå’ŒValueåˆäºŒä¸ºä¸€ï¼ŒæŒ‡å‘çš„æ˜¯åŒä¸€ä¸ªä¸œè¥¿ï¼Œä¹Ÿå³è¾“å…¥å¥å­ä¸­æ¯ä¸ªå•è¯å¯¹åº”çš„è¯­ä¹‰ç¼–ç 

  * alignment:  correlationçš„ç¨‹åº¦ï¼ˆé€šè¿‡è®¡ç®—queryå’Œæ¯ä¸ªkeyçš„ç›¸ä¼¼ç¨‹åº¦ç»™å‡ºå¯¹åº”çš„valueï¼‰å¯ä»¥é€šè¿‡å‘é‡ç‚¹ä¹˜ã€æ±‚ä¸¤è€…ä½™å¼¦ç›¸ä¼¼æ€§æˆ–è€…å¼•å…¥é¢å¤–çš„ç¥ç»ç½‘ç»œæ¥å®ç°
    $$
    Attention\space(Query, Source) = \sum_{i=1}^{L_x}Softmax(Similarity(Query,Key_i))*Value_i
    $$

* è®¡ç®—è¿‡ç¨‹å¦‚ä¸‹ï¼š![query](.\asset\query.png)
  * æ³¨æ„è®¡ç®—è¿‡ç¨‹ç¬¬ä¸€é˜¶æ®µäº§ç”Ÿç›¸ä¼¼æ€§è®¡ç®—çš„åˆ†å€¼é€šè¿‡softmaxå½’ä¸€åŒ–ç»Ÿä¸€åœ¨0-1ä¹‹é—´ï¼Œå¾—åˆ°çš„æƒé‡ç»“æœ(a)å¯¹valueè¿›è¡ŒåŠ æƒï¼Œå¾—åˆ°æœ€åçš„attentionç»“æœ

![image-20231106162638387](.\asset\20231106_attentionQKV.png)

* ä¸Šè¿°å›¾ä¸­å±•ç¤ºçš„alignmentè®¡ç®—å°±æ˜¯qå’Œkç›¸ä¹˜è¿™ç§å½¢å¼ç›¸ä¼¼åº¦ï¼Œåç»­å½’ä¸€åŒ–ä¹‹åä½œä¸ºè¾“å‡ºvalueçš„æƒé‡å€¼
* æ³¨æ„è¿™é‡ŒK æ˜¯ä¸€è¡Œæ˜¯ä¸€ä¸ªè¾“å…¥æ ·æœ¬ï¼ˆè€Œä¸æ˜¯ä¸€åˆ—ï¼‰ï¼Œæ³¨æ„è§‚å¯Ÿè¿™é‡Œçš„Kå’ŒVéƒ½æ˜¯é€šè¿‡è¾“å…¥

###### self-attention

* **key valueå’Œquery**éƒ½æ¥è‡ªäºä¸€ä¸ªinput vectorï¼Œç›¸æ¯”attentionæ˜¯æ”¹å˜äº†queryçš„è®¡ç®—æ–¹å¼ï¼ˆä¹Ÿé€šè¿‡è¾“å…¥å¾—åˆ°
  $$
  ä»çŸ©é˜µè§’åº¦çš„å…¬å¼ï¼šZ=softmax(Q^T*K)*V
  $$
  

  ![image-20231106164120627](.\asset\20231106_self_attention2.png)

  * åœ¨ä¸€å±‚ä¸­ï¼Œè‡ªæ³¨æ„åŠ›æ˜¯globalï¼Œ CNNæ˜¯localï¼›ï¼ˆæƒ³ç”¨attentionæ¥åšfully connected layerï¼‰![image-20240114165045519](.\asset\self_attention_CNN.png)

![image-20231106163451095](.\asset\20231106_self_attention.png)

###### positional encoding 

* è§£å†³orderçš„é—®é¢˜  ï¼ˆè¦çŸ¥é“ç°åœ¨å¤„ç†çš„è¯åœ¨ä¸€å¥è¯ä¸­çš„ä½ç½®ï¼Œå› ä¸ºself-attentionçš„è¿ç®—æ˜¯æ— å‘çš„ï¼Œæ— æ³•åˆ†è¾¨ï¼‰

  ![image-20231106164712619](.\asset\20231106_position_encoding.png)

  * éœ€è¦æŠŠæ‰€æœ‰çš„ä½ç½®ä¿¡æ¯éƒ½å­˜åœ¨äº†encoderé‡Œé¢
  * å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦è¿™æ ·ä¸€ç§ä½ç½®è¡¨ç¤ºæ–¹å¼ï¼Œæ»¡è¶³äºï¼š
    ï¼ˆ1ï¼‰å®ƒèƒ½ç”¨æ¥è¡¨ç¤ºä¸€ä¸ªtokenåœ¨åºåˆ—ä¸­çš„ç»å¯¹ä½ç½®
    ï¼ˆ2ï¼‰åœ¨**åºåˆ—é•¿åº¦ä¸åŒçš„æƒ…å†µ**ä¸‹ï¼Œä¸åŒåºåˆ—ä¸­tokençš„ç›¸å¯¹ä½ç½®/è·ç¦»ä¹Ÿè¦ä¿æŒä¸€è‡´
    ï¼ˆ3ï¼‰å¯ä»¥ç”¨æ¥è¡¨ç¤ºæ¨¡å‹åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»æ¥æ²¡æœ‰çœ‹åˆ°è¿‡çš„å¥å­é•¿åº¦ã€‚
  * éœ€è¦ä¸€ä¸ª**æœ‰ç•Œåˆè¿ç»­çš„å‡½æ•°ï¼Œæœ€ç®€å•çš„**ï¼Œæ­£å¼¦å‡½æ•°sinå°±å¯ä»¥æ»¡è¶³è¿™ä¸€ç‚¹
    * å‘¨æœŸæ€§ä½¿å¾—æ¨¡å‹å¯ä»¥å¤„ç†æ¯”è®­ç»ƒæ—¶æ›´é•¿çš„è¾“å…¥åºåˆ—**ï¼ˆå¤–æ¨æ€§ï¼‰**ï¼›ç¼ºç‚¹æ˜¯ä¸å¯å­¦ä¹ ï¼Œå¤–æ¨æ€§è¿˜ä¸å¤Ÿä¼˜ç§€
  * åœ¨Transformerçš„è®ºæ–‡ä¸­ï¼Œæ¯”è¾ƒäº†ç”¨positional encodingå’Œlearnable position embedding(è®©æ¨¡å‹è‡ªå·±å­¦ä½ç½®å‚æ•°ï¼‰ä¸¤ç§æ–¹æ³•ï¼Œå¾—åˆ°çš„ç»“è®ºæ˜¯ä¸¤ç§æ–¹æ³•å¯¹æ¨¡å‹æœ€ç»ˆçš„è¡¡é‡æŒ‡æ ‡å·®åˆ«ä¸å¤§ã€‚ä¸è¿‡åœ¨åé¢çš„**BERTä¸­ï¼Œå·²ç»æ”¹æˆç”¨learnable position embeddingçš„æ–¹æ³•äº†**ï¼Œä¹Ÿè®¸æ˜¯å› ä¸ºpositional encodingåœ¨è¿›attentionå±‚åä¸€äº›ä¼˜å¼‚æ€§è´¨æ¶ˆå¤±çš„åŸå› ï¼ˆçŒœæƒ³ï¼‰ã€‚

###### multi-head

 åœ¨ä¸åŒçš„channelä¸Šåšattention  

* **ä¸ºæ³¨æ„åŠ›å±‚æä¾›äº†å¤šä¸ªâ€œè¡¨ç¤ºå­ç©ºé—´â€**ã€‚ å¯¹äºå¤šå¤´æ³¨æ„åŠ›ï¼Œæˆ‘ä»¬ä¸ä»…æœ‰ä¸€ä¸ªï¼Œè€Œä¸”è¿˜æœ‰å¤šç»„Query/Key/Valueæƒé‡çŸ©é˜µï¼Œè¿™äº›æƒé‡çŸ©é˜µé›†åˆä¸­çš„æ¯ä¸€ä¸ªéƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚ ç„¶åï¼Œåœ¨è®­ç»ƒä¹‹åï¼Œæ¯ç»„ç”¨äºå°†è¾“å…¥EmbeddingæŠ•å½±åˆ°ä¸åŒçš„è¡¨ç¤ºå­ç©ºé—´ä¸­ã€‚

![image-20231106164408040](.\asset\20231106_multi_head.png)

* attentionæœ¬èº«ä¸æ˜¯æå–æ•°æ®æœ¬èº«çš„ç‰¹å¾ï¼Œ**è€Œæ˜¯ç®—å…³è”**ï¼›æ˜¯ä»generalæ„ä¹‰ä¸Šæ˜¯å¯è§£é‡Šçš„
* q kå°±æ˜¯Learnableçš„featureï¼Œattentionå°±æ˜¯å»ºç«‹ä¸¤è€…çš„å…³è”ï¼Œç„¶åå¯¹valueåšåŠ æƒ
* learnèƒ½åšçš„äº‹æ˜¯æœ¬èº«å°±å¯ä»¥è§£é‡Šçš„ï¼›
  * å³ä½¿ç†è®ºä¸ŠMLPå¯ä»¥å®ç°æ‰€æœ‰ï¼Œä½†æ˜¯å°±æ˜¯æœ‰ç®—åŠ›é™åˆ¶
  * æ‰€ä»¥é­”æ”¹çš„åœ°æ–¹å°±æ˜¯åŠ ä¸Šä¸€äº›å¯è§£é‡Šçš„ä¸œè¥¿

#### 4. Transformer VS RNN

**RNNs**

* **Pros:** LSTMs work reasonably well for long sequences.

* **Cons:**
  *  Expects an ordered sequences of inputs
  * Sequential computation: subsequent hidden states can only be computed after the previous ones are done.

**Transformer**

* **Pros:**
  * Good at long sequences. Each attention looks at all inputs.
  * Can operate over unordered sets or ordered sequences with positional encodings.
  * Parallel computation: All alignment and attention scores for all inputs can be done in parallel.

* **Cons:** Require large memory: *N x M* **alignment** and **attention scalers** are calculated and stored for a single self-attention head. 

![image-20231106164254566](.\asset\20231106_transformer2.png)

* æ•´ä¸ªç»“æ„çš„æ­å»ºï¼šä¸æ˜¯RNNï¼Œæ²¡æœ‰hidden layerçš„æ¦‚å¿µ

![image-20231106152437052](.\asset\20231106_intuitive.png)



### 5. Advanced: ViT or CNN 

![image-20231106165157063](.\asset\20231106_advance.png)

#### ViT

é¦–æ¬¡ç”¨äºè§†è§‰ä»»åŠ¡

è¯æ˜è§†è§‰ä»»åŠ¡å¯ä»¥ä¸é€‚ç”¨CNN

é—®é¢˜ï¼šå¤æ‚åº¦

global attention

#### Swin Transformer ï¼šTailored ViT

è§†è§‰æ¯”èµ·è¯­è¨€æ›´å¤æ‚çš„éƒ¨åˆ†ï¼š

* scale

åšæ³•ï¼š

* window-based attention

ä¸åŒå±‚çº§ä¹‹é—´ä½¿ç”¨ä¸åŒåˆ†è¾¨ç‡çš„patternï¼Œæ¯ä¸€å±‚ä¸­åªä½¿ç”¨ä¸¤ç§patternï¼Œäº¤æ›¿è®¡ç®—é˜²æ­¢è¾¹ç•Œä¸è¿ç»­çš„æƒ…å†µ

patch partition  + linear embedding

é™ä½åˆ†è¾¨ç‡ï¼Œæé«˜attention windowçš„æ„Ÿå—é‡

rethink: swin transformer â€”â€” is CNN locality come back? 

* é‡æ–°å¸¦æ¥äº†locality



**Recent renaissance of CNNs**

2022 google é‡æ–°ç”¨CNNæ‰“è´¥äº†vit

convnext

* modernize a standards ConvNet 

google deepmind ï¼š convnet match vision transformers at scale

* "compute is all you need?" : vit or cnn ä¸é‡è¦ï¼Œcapacityæœ€é‡è¦
* local connection



**Conclusion: ViT or CNN? -- remember the core insights **

* spatial coherence

  * local connectivity
  * global long-term depency

* invariance/equivariance

  * multi scale
  * reanslation/rotation inva/equivariant operators
  * attention-based mechanism

