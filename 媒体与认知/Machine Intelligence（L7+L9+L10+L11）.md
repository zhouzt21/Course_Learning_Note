# Machine Intelligence (L7+L9+L10+L11)

## 0 Outline

L7 Classical: Object detection (CNN-based)

* CNN-based object detection: picture
* RNN architecture: video

1. 评价标准
2. Regoin proposals
3. RCNN家族
   RCNN - Fast RCNN
   Fast RCNN --Faster RCNN
   Yolo
   Anchor-based detectors VS anchor-free detectors

L9 **GNN ** (Graph)

1. Graph, graph notations & graph data representation

2. Building neural network for graph --- the GNN

3. Treating GNN in another aspect --- the Spectral Graph CNN

   Laplacian Analysis of Graph
   Graph Fourier Transform，GFT

4. A glare of modern GNN architectures.

L10 **AI Frontiers**:  rethink ANN

1. Self-Supervised Learning
2. Contrastive Learning
   Encoder
   Loss Function
   SimCLR: a simple framework for contrastive learning
   MoCo 
   BYOL
   Barlow Twins
3. LLM: GPT VS BERT

L11 **AI Frontiers**:  beyond ANN

1. SNN

2. ONN

---

## L7 Object detection

### 1评价标准

* IoU
* Precision & Recall![image-20231030140723817](.\asset\20231030_precision_recall.png)

* mAP![image-20231030140846320](.\asset\20231030_mAP.png)
  * 每一种类别都计算AP，然后取平均

### 2 Regoin proposals

比滑窗法在不同位置和大小的穷举，候选区域算法**将像素分配到少数的分割区域**中。所以最终候选区域算法产生的数量比滑窗法少的多，从而大大减少运行物体识别算法的次数。同时候选区域算法所选定的范围天然兼顾了不同的大小和长宽比。

传统的方法中最优的：选择性搜索

* 选择性搜索算法使用《Efficient Graph-Based Image Segmentation》论文里的方法产生初始的分割区域作为输入，通过下面的步骤进行合并：
  1. 首先将所有分割区域的外框加到候选区域列表中
  2. 基于相似度合并一些区域（颜色、纹理、大小和形状交叠，四个特征权重相加）
  3. 将合并后的分割区域作为一个整体，跳到步骤1

![image-20231030145330909](.\asset\20231030_region_proposal.png)



### 3 RCNN家族

#### RCNN - Fast RCNN

RCNN使用的region proposal基于传统方法，需要对得到的特征图的Regoin of interest区域进行适当的裁剪（IoU池化），然后对于每一个区域用卷积神经网络来前传(forward) ，使用卷积层是为了从原始图像中获得特征图（一般这种卷积网络是常用的VGG Resnet等）

![image-20231030145107992](.\asset\20231030_RCNN_to_Fast_RCNN.png)

而fast RCNN的特征图是共享的(generated by backbone networks)，是先对整张图进行卷积然后再裁剪特征图前传给分类网络。

#### Fast RCNN --Faster RCNN

![image-20231030144738700](.\asset\20231030_faster_RCNN.png)



![image-20231030145853482](.\asset\20231030_faster_RCNN_two_stage.png)



![image-20231030145950254](.\asset\20231030_two_stage_to_one_stage.png)



#### YOLO

![image-20231030151113047](.\asset\20231030_YOLO.png)



#### Anchor-based detectors VS anchor-free detectors

* anchor-based的代表算法有：faster r-cnn、ssd、retinaNet、yolo v2、yolo v3…

* anchor-free的代表算法有：yolo v1、CornerNet、CenterNet…

  **anchor-base**

  * anchor就是事先通过手工或聚类方法设定好的具有不同尺寸、宽高比的方框
  * One-Stage
    * 思想是将图像划分为S×S的小格子，直接在特征图上对anchor进行回归以及类别预测。
    * 这样的做法避免了随机搜索得到region proposal，也避免了重复卷积运算，所以计算效率很高，检测速度非常快
    * 但是由于缺乏搜索region proposal（即背景筛选），导致精度不是很高
    * 而且图像中的小物体或者图像中挨得很近的多个物体会造成漏检。
    * 随着one-stage方法的不断优化迭代，很多性能优异的Backbone和Tricks被提出，比如CSPDarknet、各种数据增强、FPN（特征金字塔网络）、SPP等等，他们都可以在牺牲些许检测速度的同时提升检测精度。现在one-stage方法的精度已经达到了与two-stage具有竞争性的结果。
  * Two-Stage
    * Faster r-cnn 的出现确立了two-stage、anchor-based 检测器的主导地位。
    * Faster r-cnn由一个 rpn 网络和一个 region-wise 的预测网络（R-CNN）组成，预测目标。之后，人们又提出了许多的算法来提升其表现，包括结构重新设计、注意力机制、多尺度训练、训练策略和损失函数、特征融合和增强、候选框平衡等。目前在标准的检测基准上，SOTA的结果仍然被双阶段anchor-based方法统治。two-stage方法要筛选和优化的anchors数量要远超one-stage方法，筛选步骤较为严谨，所以耗费时间要久一些，但是精度要高一些。

  **anchor-free**

  * 由于FPN和Focal Loss的出现，有效解决了特征语义信息不足和正负样本不均衡的问题，又涌现出了一批anchor-free的算法，使得目标检测的流程进一步精简，减少了相关超参数，使得网络搭建训练更简便，泛化能力也更强。
    * 两种不同的方式检测物体，一种是首先定位到多个预定义或自学习的关键点，然后约束物体的空间范围，称为Keypoint-based方法；
    * 另一种是利用中心点或中心目标区域来定义正样本，然后预测其到目标四个边的距离，称为Center-based方法。

  * KeyPoint-based

    * 首先定位到预先定义或自学习的关键点，然后生成边框来检测物体。CornerNet 通过一对关键点（左上角和右下角）来检测物体的边框，CornerNet-Lite 引入了 CornerNet-Saccade 和 CornerNet-Squeeze 来提升其速度。Grid R-CNN 的第二个阶段利用FCN的位置敏感的优点来预测网格点，然后再判断边框、定位物体。ExtremeNet 检测物体的4个点（最上面、最左面、最下面、最右面）以及一个中心点来生成物体的边框。Zhu 等人利用关键点估计来找到物体的中心点，然后回归出其他的属性，包括大小、三维位置、朝向、姿态等。CenterNet 扩展了CornerNet，通过三个点而不是两个点来提升精度和召回率。RepPoints 将物体表示为一个样本点的集合，通过约束物体的空间范围、强调语义重要的局部区域来学习。

  * Center-based
    这类方法将物体的中心区域（中心点或区域）看作为前景，定义正样本，然后预测它到物体四个边的距离。YOLO 将图像分割为S×S个网格，如果物体的中心点落在了某网格内，该网格就要负责检测该目标物体。DenseBox 利用物体中心位置的圆圈区域来定义正样本，然后预测该圆圈到物体边界的四个距离。GA-RPN 将物体中心区域的像素点定义为正样本，为 Faster R-CNN 预测物体候选框的位置、宽度和高度。FSAF 在 RetinaNet 之上加了一个 anchor-free 的分支和在线特征选取机制，这个分支将物体的中心区域定义为正样本，利用它到物体的四个边的距离来定位它。FCOS 将物体边框内的所有位置都定义为正样本，然后通过4个距离值和1个 centerness 分数来检测物体。CSP 只将物体的中心点定义为正样本，通过固定的 aspect ratio 来检测行人。FoveaBox 将物体的中间部分位置定义为正样本，每个位置有4个距离来进行检测。

    原文链接：https://blog.csdn.net/Just_do_myself/article/details/118520732

    不固定的话一般使用 heirachy  ( CNN 中的 multi scale)

![image-20231030153005046](.\asset\20231030_anchor_based_detectors.png)

pipeline:

![image-20231030152334430](.\asset\20231030_pipeline.png)

paper:

![image-20231030152741029](.\asset\20231030_paper.png)



occlusions & partial visibility

* multi-scale feature extraction 不同尺度
* nms 非极大值抑制 ： 解决redundant问题
* fine-tuning and data augmentation 数据增强
* keypoints and parts detection 关键点/部分的监测
* attention mechanisms
* data labeling 
* sensor fusion   
* temporal information 

-----



## L9 GNN

### 1. Graph, graph notations & graph data representation

* 图信息的表示![image-20231113141153016](.\asset\20231113_graph_signal.png)

* 图相关的任务![image-20231113142655280](.\asset\20231113_graph_task.png)
  * 视觉中的图像每一个像素是一个节点，分别通过边和其他节点相连，每一个节点是一个RGB三维向量

  * 语言中的每一个单词是一个节点，通过边和其他节点相连接，邻接矩阵式一个只有次对角线非零的矩阵，因为每一个单词之和相邻两个单词相连

* ![image-20231113143733542](.\asset\20231113_graph_task2.png)

### 2. Building neural network for graph --- GNN

* **GNN**: “**graph-in, graph-out**” architecture

  * 最简单的GNN是每一个图的组成部分都是一个MLP网络（节点和边）

* **GNN Predictions by Pooling Information**

  * How to **collect info. from edges** and give them to nodes for prediction? --> **pooling**
    * neighborhood-based pooling operation

  * 任务会有：知道点求边、知道边求点、知道点求全局等等，所以需要通过一种方式来使得信息能顺畅地在不同属性之间传递

* By stacking message passing GNN layers together, a node can eventually incorporate information from across the entire graph

  * Message passing as convolution (Message passing can occur between **either nodes or edges**)
  * **GCN**:  更新图中节点的表征——通过**pooling neighboring nodes** at a distance
    * 信息传递不一定是同样的size和shape，可以根据不同组成成分学习出分别的线性映射![image-20231113150946536](.\asset\20231113_GNN_update.png)


  * GNN不是简单参数越多表现越好，经验上节点上的信息越多越好![image-20231113150838427](.\asset\20231113_GNN_performance.png)

  * ·同样也是cnn中pooling出现的问题——如何选取一个好的算子来聚集周边信息![image-20231113150817753](.\asset\20231113_risk.png)

* 上述做法都是基于图的操作，基本是基于neighborhood-based pooling operation这一个工具，实际上还可以用上更多CNN的方法

### 3. Treating GNN in another aspect --- the Spectral Graph CNN

**Convolutions** & **frequency domain analysis** may further empower GNNs

* 实际上图的性质可以简化为矩阵表示，特征值和特征向量有很多可以使用的

#### **Laplacian Analysis of Graph**

* recall

  * $$
    拉普拉斯算子：\space ∆f= f'(x) − f'(x-1) = f(x+1) + f(x-1) - 2 ∗ f(x)
    $$

* 记住结论：拉普拉斯矩阵恰好是度矩阵减去邻接矩阵![image-20231113152011090](.\asset\20231113_GNN_matrix.png)

* 结论为：拉普拉斯算子的作用结果是当前点对其所有自由度上的微扰的增益![image-20231113152201101](.\asset\20231113_GNN_matrix2.png)

![image-20231113153716063](.\asset\20231113_laplacian_matrix.png)

#### **Graph Fourier Transform**，GFT

* 无向图的拉普拉斯矩阵是对称阵，这时候可以将对角化
* ![image-20231113153832108](.\asset\20231113_laplacian_matrix2.png)

使用卷积定理，互发现关键要学的是h_thetha这个滤波器，它是对角阵![image-20231113154227447](.\asset\20231113-spectrum.png)

使用多项式来描述一个对角阵，则参数变成了k个，比起整个矩阵来说大大减少。

![image-20231113154339168](.\asset\20231113_simplified.png)

U是拉普拉斯的特征矩阵，这样实现了拉式算子和图卷积复合操作作用于图像。

也可以使用切比雪夫多项式逼近。

![image-20240114184825823](.\asset\切比雪夫.png)

### 4. A glare of modern GNN architectures.

* Graph Attention Networks --- GAT
  * GAT provides solutions using **attention coefficient** in **message aggregation**
  
  * attention coefficient: a measurement of how **relevant (important) a neighboring node is in relation to the center node**
  
  * directed graph friendly: attention coefficient e_1,2 ≠ e_2,1
  
    * $$
      e_{i,j}=a([Wh_i||Wh_j]), j\in N_i\\
      W: 共用权重线性映射, for feature augmentation in the nodes\\
      N_i: 节点i的所有邻居\\
      [∗ || ∗]: 连接点两边的特征, 注意有顺序区别\\
      a(∗): 投影高维数据到一个标量
      $$
  
    * $$
      a_{i,j}=\frac{exp(LeakyReLU(e_{i,j}))}{\sum_{k\in N_i} exp(LeakyReLU(e_{i,k}))}
      $$
  
  * Instead of equal aggregation in GNN, GAT using a_i,j  as weights in aggregation，实际是借用了attention处理权重的那一套——计算相关性，然后非线性再归一化，形成的权重系数加权；这里是用在了aggregation上
  
    * $$
      ℎ^′_i = Actication(\sum_{j \in N_i} a_{i,j}Wh_{i,j})\\
      ℎ^′_i 表示通过attention层之后的新表征
      $$
  
* Generative modelling & GraphVAE
  * **Generative model** for graphs: generate new graphs by sampling from a learned distribution or by completing a graph given a starting point
  * Design new drugs: novel molecular graphs with specific properties

* Graph recurrent neural network --- GRNN
  * For a **time varying graph**, recurrent unit could be applied

## L10 rethinking ANN

### 1 Self-Supervised Learning

* For **supervised learning** methods, **data** becomes the bottleneck
  *  **Collection**: task specific data is hard to collect
  * **Annotation**: time-consuming and expensive

* solution: pretrain + finetune

  * Step1 - Pretrain: train model on **existing, related, large-scale** dataset
  * Step2 - Finetune: train model on **task-specific, small** dataset
    * **E.g., Object Detection:** **Pretrain** on **ImageNet** dataset for **classification** only,Finetune on the small dataset for both **classification** and **bbox regression**

  * **“universal” feature representation** can be learned by pretraining the earlier layers of networks, and then **transferred** to different downstream tasks (makes the model converge faster)

  * Problems: Still hungry for annotated data:
    * time-consuming and expensive (2D/3D annotation)
    * introduce human bias and error
    *  need expert knowledge

* solution:  Learn via **observation and interaction without explicit annotation**
  * Learn by itself on unannotated data
  *  **Pretrain** the model on a pretext task to learn something useful on **large-scale** data **without annotation**
  * **Finetune** on downstream task with specific/small dataset
    * learn with labels from annotation free pretext tasks
    * Lables have to be generated automatically, not mannully annotated

some example:

![image-20240114191837794](.\asset\rotation_predicted.png)

* **Visualization** **of the attention map**
  * Self-supervised learning achieves **sensible focus region** in attention map, **similar to** supervised learning

* **Visualization** **of the layer filters**
  * Self-supervised learning tends to have **larger variety of** **edge filters** than those learned on the supervised task 

![image-20240114204654898](.\asset\pretext_task2.png)



![image-20240114205022723](.\asset\pretext_task3.png)

![image-20240114205052233](.\asset\pretext_task4.png)



### 2 Contrastive Learning

Is there more general pretext task?

* **Problem**: Learning depends on heuristic specific pretext tasks

* **Intuition**: 
  * Translate **high dimensional data** to a **low dimensional representation**
  * **Similar input objects** are mapped to **nearby points** on a manifold(在流型上)

**Instance Invariance --> contrastive learning**

#### Encoder

Goal: learn an encoder function f that 

* **Attract positive** pairs (x, x+) together 

* **Push away negative** pairs (x, x-) in feature space 

#### **Loss Function**

Discriminate  1 positive pair from K negative pairs

InfoNCE loss is essentially a cross entropy loss for a (K+1)-way softmax classifier 

![image-20240114192224805](.\asset\contrastive_loss_function.png)



#### **SimCLR: a simple framework for contrastive learning**

![image-20240114210312672](.\asset\simclr.png)

* 我们随机采样一个包含 N 个样本的 minibatch，并基于其中增广后的样本对 定义对比预测任务，得到 2N 个数据点 (每个样本增广一次)。
* 我们没有显式地采样负样本 (negative exmples)。相反，给定一个正对，我们 将一个 minibatch 中的其他 2(N−1) 个增广后的样本视为负样本。
* 设 sim(u,v)=uTv/∥u∥∥v∥ 表示经 L2 归一化的 u 和v 之间的点积 (即 余弦相似度)。然后将正对样本 (i,j)的损失函数定义为上述式子。 其中 1[k≠i]∈{0,1}是一个指示函数，当 k≠i值为 1，否则为 0，而 τ 表示一个温度参数。最终的损失计算所有的正对，包括 (i,j) 和 (j,i)。

InfoNCE loss is proved to be a **lower bound** on **mutual information (MI)** between qi = f(x) and ki^+ = f(x^+)

The **larger** *K* is, the **tighter** the *MI* bound is, thus **a lot of negative samples needed**

#### MOCO：

![image-20240114210921113](.\asset\moco.png)

MOCO采用了一个双网络结构，包括一个在线网络（online network）和一个目标网络（target network）。其中在线网络是用于提取特征的主网络，而目标网络则根据在线网络的参数进行周期性的更新。使用动量优化器来更新目标网络的参数。l

* 基于对比的自监督学习其实就是训练一个编码器，然后在一个大的字典里确保和对应的键是相似的，和其它的是不相似的。以字典的大小就成了关键，传统的方法是字典的大小就是等于mini_batch的大小，但是这种方法由于显卡和算力的问题导致其不能太大。
* 在这里MoCo使用了队列来存储这个字典。在训练的时候，每一个新的batch完成编码后就进入队列；然后最老的那一个编码就出队列。这样保证了字典的大小和batch的大小解耦了，也就是说字典的大小可以远远大于batch的大小。
* loss function仍然采用的infoNCE loss

#### BYOL：

![image-20240114211016563](.\asset\BYOL.png)

**BYOL依赖于两个神经网络**，即在线和目标网络，它们相互作用并相互学习**。**从图像的增强视图出发，我们训练在线网络预测**同一图像在不同增强视图下**的目标网络表示。同时，我们使用在线网络的缓慢移动平均值更新目标网络

#### Barlow Twins

![image-20240114211230105](.\asset\Barlow_twin.png)

核心思想是利用数据的冗余性来学习出更好的数据表示。（通过减少数据的冗余部分，来学习到更好的表征；而这个减少冗余的方式是通过减少数据点之间的相关性，和减少与单位阵之间的距离）

* 该方法通过量化数据点之间的相关性，以此来调整神经网络的参数。
  * 最小化数据点之间的相关性矩阵与单位矩阵之间的距离来进行训练。同时，为了提高算法的稳定性和收敛性，Barlow Twins方法引入了一个尺度参数。

### 3 LLM: GPT VS BERT 

笔记见视听导笔记

## L11 beyond ANN

### 1. Brain-inspired computing， SNN

**Artificial Neuron**

for Artificial Neural Networks (ANN

**Spiking Neuron**

for Spiking Neural Networks (SNN)**

![image-20240114192602934](.\asset\SNN.png)

![image-20240114201440221](.\asset\LIF.png)

脉冲神经网络与人工神经网络究竟**有何异同**？

脉冲神经网络究竟有哪些**潜在优势**？

* **轻量化和鲁棒性优势**

* **动力学优势**
* **时序处理优势**

与成熟有效的人工神经网络 (ANN) 训练算法：**误差反向传播算法 (Back Propagation)** 不同，神经网络研究中最困难的问题之一是由于**复杂的动力学和脉冲的不可微性质**导致的训练困难。

### 2.  Optical Neural Networks (ONNs):

**AI at the speed of light**

**High-speed**, **Low loss** (低损耗), **high dimensionality** (高维度), **high** **parallelism** (高并行度)

* How to construct optical neural network
  * Free-space optics: Lensed optical system 
    * Plane wave incident（入射） at different angle = signal with different spatial frequencies 
  * Integrated photonics 集成光子电路

flourier transforming property of lenses 

#### 2.1 Free-space optics:

1. compute with 4f system:
   1.  Image filtering 
   2.  Image correlation
      * **Convolution theorem（卷积定理）**
   3. convolution

2. Lens focusing for computing

3. Diffraction itself as neural network connection
   1.  Fresnel approximation of scalar diffraction

![image-20240114201204901](.\asset\Flourier_ONN.png)

#### 2.2 Integrated ONN

* The internet is faster because of the use of fiber

![image-20240114194943519](.\asset\integratedONN.png)

* Wavelength multiplexing for higher transmission speed

* Integrated waveguides: 集成波导
  * confine(限制) light with index difference

* Even more: photonics + electronics on single chip

* Moore’s law of photonics

* Neural network: linear matrix multiplication + nonlinear activation function
  *  matrix vector multiplication (MVMs) are the most time/energy consuming operations

![image-20240114200000108](.\asset\matrix_ONN.png)

![image-20240114200046602](.\asset\matrix_ONN2.png)

![image-20240114200253385](.\asset\ONN_calculation.png)

不同波长代表不同的参数信息，最后照射到发光二极管上，形成二进制的向量

![image-20240114201105338](.\asset\ONN_pipeline.png)



**ONN advantages in computing**

* Low latency: nano (10-9) second/pico (10-12) second
* High speed: ~Peta (1015) operations per second regime
* High efficiency: single photon computing
* Optical optimization: combinatorial optimization problem
  * Conventional algorithms: time scale with the problem complexity
  * Optical optimization: time independent on the problem complexity

